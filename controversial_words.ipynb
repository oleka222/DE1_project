{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2d9ca5-8d07-408b-a62b-1e6912dcf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.90:7077\") \\\n",
    "        .appName(\"template\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 1)\\\n",
    "        .config(\"spark.driver.port\", 9998)\\\n",
    "        .config(\"spark.blockManager.port\", 10005)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfa89d0-b2c7-42b3-8185-13f10fa545ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = ss.read.json('hdfs://192.168.2.90:9000/user/ubuntu/RC_2006-03').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9a8b48-a809-4e4a-99f2-6a2f70f40cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c313fb-e66e-4b35-8d8c-c8553c2d0287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|                              body|\n",
      "+----------------------------------+\n",
      "|              It's not garbage,...|\n",
      "|              Actually, it's ni...|\n",
      "|              I've read them, a...|\n",
      "|              If you look at a ...|\n",
      "|              That's true, but ...|\n",
      "|              High school drop ...|\n",
      "|                         [deleted]|\n",
      "|              They put the powe...|\n",
      "| 手元のiCalでも化けていたので、...|\n",
      "|              or, download the ...|\n",
      "|              there's some conf...|\n",
      "|                         [removed]|\n",
      "|              I do.  See the co...|\n",
      "|                         [removed]|\n",
      "|              what exactly is i...|\n",
      "|平面上の位置、および押す強さ(?)...|\n",
      "|              I love the last s...|\n",
      "|              maybe he thought ...|\n",
      "|              I've done this.  ...|\n",
      "|              Open your wife's ...|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('body').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc01afd3-ba76-4aab-b2c8-640c0a314153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting controversial words\n",
    "cont_words = 'abuse, administration, afghanistan, aid, america,' + \\\n",
    "'american, army, attack, attacks, authorities, authority, ban, banks, benefits, bill, bills,' + \\\n",
    "'border, budget, campaign, candidate, candidates, catholic, china, chinese, church,'+ \\\n",
    "'concerns, congress, conservative, control, country, court, crime, criminal, crisis, cuts,'+\\\n",
    "'debate, debt, defense, deficit, democrats, disease, dollar, drug, drugs, economy, education,'+\\\n",
    "'egypt, election, elections, enforcement, fighting, finance, fiscal, force, funding,'+\\\n",
    "'gas, government, gun, health, immigration, inaccuracies, india, insurance, investigation,'+\\\n",
    "'investigators, iran, israel, job, jobs, judge, justice, killing, korea, labor, land,'+\\\n",
    "'law, lawmakers, laws, lawsuit, leadership, legislation, marriage, media, mexico, military,'+\\\n",
    "'money, murder, nation, nations, news, obama, offensive, officials, oil, parties,'+\\\n",
    "'peace, police, policies, policy, politics, poll, power, president, prices, primary, prison,'+\\\n",
    "'progress, race, reform, republican, republicans, restrictions, rule, rules, ruling, russia,'+\\\n",
    "'russian, school, security, senate, sex, shooting, society, spending, strategy, strike, support,'+\\\n",
    "'syria, syrian, tax, taxes, threat, trial, unemployment, union, usa, victim, victims,'+\\\n",
    "'violence, vote, voters, war, washington, weapons, world,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bac9de2-36f2-4f06-aa19-db6614ea36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting semi-controversial words\n",
    "semi_cont_words = 'account, advantage, amount, attorney, chairman,'+\\\n",
    "'charge, charges, cities, class, comment, companies, cost, credit, delays, effect, expectations,'+\\\n",
    "'families, family, february, germany, goal, housing, information, investment,'+\\\n",
    "'markets, numbers, oklahoma, parents, patients, population, price, projects, raise, rate,'+\\\n",
    "'reason, sales, schools, sector, shot, source, sources, status, stock, store, worth,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bf135d9-9923-42e5-9934-26bf8af101e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the words into list\n",
    "import re\n",
    "controversial = re.findall('\\w+', cont_words.strip().lower())\n",
    "semi_controversial = re.findall('\\w+', semi_cont_words.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a532912-300e-446b-82f7-17a9c2cbbdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating udf to tokenize text of a comment\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def tokenize(line):\n",
    "    return re.findall('\\w+', line.strip().lower())\n",
    "udf_tokenize =  udf(tokenize, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8dfd39b-82f1-427e-ba39-a90b0ed1db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying udf\n",
    "df = df.withColumn('body', udf_tokenize('body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea79488-9fc5-448d-a92d-f54eaf6d448f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|                             body|\n",
      "+---------------------------------+\n",
      "|             [it, s, not, garb...|\n",
      "|             [actually, it, s,...|\n",
      "|             [i, ve, read, the...|\n",
      "|             [if, you, look, a...|\n",
      "|             [that, s, true, b...|\n",
      "|             [high, school, dr...|\n",
      "|                        [deleted]|\n",
      "|             [they, put, the, ...|\n",
      "| [手元のicalでも化けていたので...|\n",
      "|             [or, download, th...|\n",
      "|             [there, s, some, ...|\n",
      "|                        [removed]|\n",
      "|             [i, do, see, the,...|\n",
      "|                        [removed]|\n",
      "|             [what, exactly, i...|\n",
      "|[平面上の位置, および押す強さ,...|\n",
      "|             [i, love, the, la...|\n",
      "|             [maybe, he, thoug...|\n",
      "|             [i, ve, done, thi...|\n",
      "|             [open, your, wife...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('body').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d24e0a-3ee6-45cd-ba7f-bce35546826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading english words, importing nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df445930-22fd-4424-83f4-94a648b0be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating udf to keep only english words\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "def only_english_words(line):\n",
    "    import nltk\n",
    "    return [word for word in line if wnl.lemmatize(word) in nltk.corpus.words.words()]\n",
    "\n",
    "udf_only_english_words = udf(only_english_words, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a5fb29-e76e-49ce-87e7-eaef95e8598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn('body', udf_only_english_words('body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b15c287-f5f9-4ef9-a12e-3cd4fff8390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 13:37:29 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 4) (192.168.2.251 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'nltk'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/03/01 13:37:29 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'nltk'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df2\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/home/ubuntu/spark-3.2.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'nltk'\n"
     ]
    }
   ],
   "source": [
    "df2.select('body').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60231b6d-6d15-4f2b-978f-433a3f003dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in ./.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in ./.local/lib/python3.8/site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in ./.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: regex>=2021.8.3 in ./.local/lib/python3.8/site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62676ef-b472-4d0b-8839-fa1b429e82fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86b7d2-8fe4-49c0-8eae-ad5594d1548b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e7167-e59f-4014-825f-f67ee523bc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa11b9-d89d-4a28-a5b6-0c67b10a05c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cb882-12f4-4c93-94bc-6d1367fc48dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba85ae5-b7db-499f-b76a-97ec6e374bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157df6d-7192-4c4d-8a50-f17366ddb366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e2a9f-f753-4f83-8052-d2d2f2254c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af428f9-4997-477a-ba8c-5857cfae07e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4432e-a919-47d4-af51-db230d78d0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3d9c6-666b-4703-8825-770dfcc8b2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd7227-a491-436b-8822-1d5301c40d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676ee31-b19c-45c0-a139-fa8aa39c6672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d66c0c-568c-45b6-ba81-262a27e574d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0dbbe3-44ab-4348-81f9-cea9a0a87174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
